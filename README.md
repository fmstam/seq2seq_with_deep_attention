# Sequence to sequence with attention (seq2seq-with-deep-attention)
Sequence to sequence (seq2seq) using RNN with attention mechanism in pytorch.

This work is the pytorch translation of the Loung model for sequence to sequence. We have used the date generation code from https://github.com/tensorflow/tfjs-examples/tree/master/date-conversion-attention, to create a dataset and have followed the same steps in this link, but our implementation uses NLLoss instead of cat. softmax loss function. 

## To run:
  see `example.ipynb`
## For TensorFlow impelementation and more useful linksL
  see, please check: https://github.com/AndreMaz/deep-attention


