# seq2seq-with-deep-attention
Sequence to sequence (seq2seq) using RNN with attention mechanism in pytorch.

This work is the pytorch translation of the Loung model for sequence to sequence. We have used the date generation code from https://github.com/tensorflow/tfjs-examples/tree/master/date-conversion-attention. You can see that, we have followed the same steps in this link, but our model uses NLLoss instead of cat. softmax loss function. 


